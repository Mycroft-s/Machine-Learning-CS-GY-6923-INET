{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4cdfb0dc-5e71-41b4-ba5a-35ff72991191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " id                     0\n",
      "gender                 0\n",
      "age                    0\n",
      "hypertension           0\n",
      "heart_disease          0\n",
      "ever_married           0\n",
      "work_type              0\n",
      "Residence_type         0\n",
      "avg_glucose_level      0\n",
      "bmi                  201\n",
      "smoking_status         0\n",
      "stroke                 0\n",
      "dtype: int64\n",
      "Training indices: Index([434, 725, 783, 2912, 2329, 599, 390, 2730, 2838, 3837], dtype='int64')\n",
      "Testing indices: Index([184, 47, 1746, 3307, 2615, 1406, 1853, 2902, 2829, 2713], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data_path = r'C:\\Users\\Admin\\Desktop\\ML online\\final project\\healthcare-dataset-stroke-data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values in each column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Fill missing values in 'bmi' column\n",
    "data['bmi'] = data['bmi'].fillna(data['bmi'].mean())\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(data[['age', 'avg_glucose_level', 'bmi']])\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['id', 'stroke'])\n",
    "y = data['stroke']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"Training indices:\", X_train.index[:10])\n",
    "print(\"Testing indices:\", X_test.index[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d819d7cd-625b-4c79-bae6-dab7dd9b20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 1.2670\n",
      "Best model saved at epoch 1 with loss 1.2670\n",
      "Epoch 2, Loss: 1.1575\n",
      "Best model saved at epoch 2 with loss 1.1575\n",
      "Epoch 3, Loss: 1.1122\n",
      "Best model saved at epoch 3 with loss 1.1122\n",
      "Epoch 4, Loss: 1.1048\n",
      "Best model saved at epoch 4 with loss 1.1048\n",
      "Epoch 5, Loss: 1.0648\n",
      "Best model saved at epoch 5 with loss 1.0648\n",
      "Epoch 6, Loss: 0.9658\n",
      "Best model saved at epoch 6 with loss 0.9658\n",
      "Epoch 7, Loss: 0.9244\n",
      "Best model saved at epoch 7 with loss 0.9244\n",
      "Epoch 8, Loss: 0.9000\n",
      "Best model saved at epoch 8 with loss 0.9000\n",
      "Epoch 9, Loss: 0.8781\n",
      "Best model saved at epoch 9 with loss 0.8781\n",
      "Epoch 10, Loss: 0.8660\n",
      "Best model saved at epoch 10 with loss 0.8660\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "import numpy as np\n",
    "\n",
    "# Define Transformer-based binary classifier with increased model complexity\n",
    "class TransformerBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TransformerBinaryClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 256)  # Increase embedding dimension\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=256, nhead=8, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=8)  # Increase Transformer layers\n",
    "        self.fc = nn.Linear(256, 1)  # Output one value for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = torch.tensor(x, dtype=torch.float32) if not isinstance(x, torch.Tensor) else x\n",
    "        x = self.embedding(x).unsqueeze(1)  # Shape: (batch_size, 1, d_model)\n",
    "        x = self.transformer_encoder(x)  # Output shape: (batch_size, 1, d_model)\n",
    "        x = x.mean(dim=1)  # Apply mean pooling\n",
    "        return self.fc(x)  # Shape: (batch_size, 1)\n",
    "\n",
    "# Prepare data and apply oversampling to balance classes\n",
    "ros = RandomOverSampler()\n",
    "X_resampled, y_resampled = ros.fit_resample(X_train, y_train)\n",
    "\n",
    "# Create DataLoader with resampled data\n",
    "train_dataset = TensorDataset(torch.tensor(X_resampled.values, dtype=torch.float32), torch.tensor(y_resampled.values, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Compute class weights and apply to loss function\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=np.array([0, 1]), y=y_train)\n",
    "class_weights_tensor = torch.tensor([class_weights[1]], dtype=torch.float32)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=class_weights_tensor)\n",
    "\n",
    "# Initialize model, optimizer with reduced learning rate, and learning rate scheduler\n",
    "input_dim = X_train.shape[1]\n",
    "model = TransformerBinaryClassifier(input_dim=input_dim)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)  # Reduce initial learning rate\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)  # Decay learning rate every 5 epochs\n",
    "\n",
    "# Training function with best model saving\n",
    "def train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10):\n",
    "    model.train()\n",
    "    best_loss = float('inf')\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)  # Shape: (batch_size,)\n",
    "            loss = criterion(outputs, labels)  # Binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        # Step scheduler and save best model\n",
    "        scheduler.step()\n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch + 1}, Loss: {epoch_loss:.4f}\")\n",
    "        \n",
    "        # Save best model based on loss\n",
    "        if epoch_loss < best_loss:\n",
    "            best_loss = epoch_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pth')\n",
    "            print(f\"Best model saved at epoch {epoch + 1} with loss {best_loss:.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, scheduler, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5f2877ca-c804-4ca8-a923-78299fb59273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal threshold: 0.9777963757514954\n",
      "Binary Classification Metrics at Optimal Threshold:\n",
      "Accuracy: 0.9384\n",
      "Precision: 0.3023\n",
      "Recall: 0.2826\n",
      "F1 Score: 0.2921\n",
      "AUC-ROC: 0.8519\n",
      "AUC-PR: 0.2066\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, average_precision_score, precision_recall_curve\n",
    "\n",
    "# 确保 y_test 是二维的\n",
    "if y_test.ndim == 1:\n",
    "    y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# 测试数据加载器\n",
    "test_dataset = TensorDataset(torch.tensor(X_test.values, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 模型评估\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 获取预测概率\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.sigmoid(outputs).squeeze(1)  # 转换为概率\n",
    "        all_preds.extend(probs.cpu().numpy())\n",
    "        all_labels.extend(labels.cpu().numpy().flatten())\n",
    "\n",
    "all_preds = np.array(all_preds)\n",
    "all_labels = np.array(all_labels)\n",
    "\n",
    "# 优化阈值，避免计算过程中出现 NaN\n",
    "def optimize_threshold(y_true, y_pred_probs):\n",
    "    precisions, recalls, thresholds = precision_recall_curve(y_true, y_pred_probs)\n",
    "    f1_scores = np.zeros_like(precisions)  # 初始化 F1 分数数组\n",
    "    \n",
    "    # 计算 F1 分数，仅在 precisions + recalls 非 0 时计算\n",
    "    for i in range(len(f1_scores)):\n",
    "        if precisions[i] + recalls[i] != 0:\n",
    "            f1_scores[i] = 2 * (precisions[i] * recalls[i]) / (precisions[i] + recalls[i])\n",
    "    \n",
    "    # 找到 F1 分数最高的阈值索引\n",
    "    optimal_idx = np.argmax(f1_scores)\n",
    "    return thresholds[optimal_idx] if optimal_idx < len(thresholds) else 0.5  # 如果出界则返回默认阈值\n",
    "\n",
    "# 找到最佳阈值\n",
    "optimal_threshold = optimize_threshold(all_labels, all_preds)\n",
    "print(f\"Optimal threshold: {optimal_threshold}\")\n",
    "\n",
    "# 使用最佳阈值生成二进制预测\n",
    "binarized_preds = (all_preds >= optimal_threshold).astype(int)\n",
    "\n",
    "# **二分类评估指标**\n",
    "accuracy = accuracy_score(all_labels, binarized_preds)\n",
    "precision = precision_score(all_labels, binarized_preds, zero_division=1)\n",
    "recall = recall_score(all_labels, binarized_preds)\n",
    "f1 = f1_score(all_labels, binarized_preds)\n",
    "auc_roc = roc_auc_score(all_labels, all_preds)  # AUC-ROC\n",
    "average_precision = average_precision_score(all_labels, all_preds)  # AUC-PR\n",
    "\n",
    "print(\"Binary Classification Metrics at Optimal Threshold:\")\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"AUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"AUC-PR: {average_precision:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d19e58a7-ebfb-49a4-bd32-87a52461ac94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nMachine Learning Project Report: Stroke Prediction Model Using Transformer-Based Binary Classifier\\nProject Objective\\nThe objective of this project is to build a machine learning model to predict the likelihood of a stroke in a given patient based on a set of medical and demographic features. \\nWe use a Transformer-based binary classifier, trained on a dataset of patient data, to classify each patient into \"stroke\" or \"no stroke\" categories.\\n\\n\\nDataset Description\\nThe dataset contains information on patient demographics and health metrics relevant to stroke prediction. \\nThe features include:\\n\\nPatient ID: Unique identifier for each patient.\\nGender: Patient gender (e.g., Male, Female).\\nAge: Patient age.\\nHypertension: Whether the patient has hypertension (1) or not (0).\\nHeart Disease: Whether the patient has any heart disease (1) or not (0).\\nEver Married: Whether the patient has ever been married.\\nWork Type: Employment type (e.g., Private, Self-employed).\\nResidence Type: Residence location (Rural or Urban).\\nAverage Glucose Level: Average glucose level in the blood.\\nBMI: Body Mass Index of the patient.\\nSmoking Status: Smoking behavior (e.g., never smoked, smokes).\\nStroke (target): Whether the patient had a stroke (1) or not (0).\\n\\n\\nData Preprocessing\\nData preprocessing steps include handling missing values, encoding categorical features, \\nand scaling numerical features to prepare the data for training.\\n\\nHandling Missing Values: \\nWe identified and filled missing values in the \\'BMI\\' column with the column mean.\\nEncoding Categorical Features: \\nLabel encoding was applied to categorical columns such as \\'gender\\', \\'ever_married\\', \\'work_type\\', \\'Residence_type\\', and \\'smoking_status\\'.\\nScaling Numerical Features: \\nStandard scaling was applied to numerical columns (age, avg_glucose_level, and bmi) to normalize the data and enhance the model\\'s learning stability.\\nFeature-Target Split: \\nWe dropped the \\'id\\' and \\'stroke\\' columns to create feature matrix X and target vector y. The \\'stroke\\' column serves as the target variable.\\n\\nTrain-Test Split\\nTo evaluate the model’s performance, the dataset was split into training and testing sets with a ratio of 80:20. This ensures that 80% of the data is used for training, and 20% is held back for evaluation.\\n\\n\\nModel Architecture\\nWe implemented a binary classifier based on a Transformer architecture to capture complex patterns in the data.\\n\\nEmbedding Layer: The input features are transformed into a higher-dimensional representation through an embedding layer (nn.Linear(input_dim, 64)), \\nwith 64 being the transformer’s model dimension.\\nTransformer Encoder: We used four layers of Transformer encoders with eight attention heads each. \\nThis encoder learns contextual relationships between features.\\nFully Connected Layer: The final layer (nn.Linear(64, 1)) outputs a single logit, \\nwhich is then passed through a sigmoid function during evaluation to represent the probability of a stroke.\\nBinary Cross-Entropy Loss: Since this is a binary classification problem, we used BCEWithLogitsLoss, \\nwhich combines the sigmoid activation and binary cross-entropy loss functions, \\nallowing us to work directly with logits for increased numerical stability.\\nModel Training\\nThe model was trained using the Adam optimizer, a batch size of 32, and a learning rate of 0.001 over 10 epochs. \\nDuring each epoch, the model processed batches of input data, computed the loss, and adjusted the weights accordingly to minimize the binary cross-entropy loss.\\n\\nModel Evaluation\\nWe evaluated the model using both binary and multi-label metrics to assess performance. These included:\\n\\nBinary Classification Metrics:\\n\\nAccuracy: Overall correctness of predictions.\\nPrecision: Correctness of positive predictions.\\nRecall: Sensitivity of the model to actual positive cases.\\nF1 Score: Harmonic mean of precision and recall.\\nAverage Precision: Area under the precision-recall curve, capturing the model’s ability to predict positive instances accurately.\\nMulti-label Metrics (simulated for binary classification):\\n\\nHamming Loss: Measures the fraction of incorrect predictions in the binary format.\\nOne Error: Checks if the highest-ranked prediction is an actual positive.\\nRanking Loss: Measures if true labels are ranked above false labels.\\nCoverage: Measures the range needed to cover all true labels in the ranked predictions.\\n\\nObservations from Model Evaluation\\nBinary Metrics:\\nThe model achieved a high accuracy score, indicating it correctly identified the majority of cases. \\nHowever, precision and recall were imbalanced, likely due to the presence of fewer positive cases (stroke instances) in the dataset.\\nRecall and F1 Score were low, suggesting the model struggles to detect true positive cases (i.e., stroke cases).\\nMulti-label Metrics:\\nHamming Loss was significant, indicating a fair amount of prediction error in binary classifications.\\nOne Error and Ranking Loss yielded reasonable scores, suggesting the model ranked relevant labels appropriately when present.\\nCoverage was negative, possibly due to sparse true positive cases.\\n\\n\\nConclusion\\nThe Transformer-based model demonstrated good performance on the negative cases but struggled with positive cases, \\nwhich is evident from the low recall score. This outcome indicates that the model is likely biased toward predicting the majority class. This issue is common in imbalanced datasets, where the model learns to prioritize the majority class. Possible next steps for improvement include:\\n\\nClass Balancing: Apply techniques such as class weighting or oversampling of the minority class to improve model sensitivity to positive cases.\\nThreshold Adjustment: Experiment with different thresholds to find the optimal decision boundary for stroke prediction.\\nSimpler Models for Comparison: Testing simpler models, such as logistic regression or decision trees, \\ncould provide a useful baseline to compare the performance of the Transformer-based model. \\nIn some cases, simpler models can perform as well as or better than complex models, \\nespecially when the dataset is limited in size or heavily imbalanced.\\n\\nFurther Hyperparameter Tuning: Experimenting with hyperparameters such as the number of Transformer encoder layers, \\nthe embedding size, and the number of attention heads may lead to performance improvements. Additionally, adjusting the learning rate, \\nbatch size, or the number of epochs could help optimize the model’s learning process.\\n\\nEnsemble Methods: Implementing an ensemble of models (e.g., combining Transformer with simpler models) \\ncould potentially enhance performance by capturing different aspects of the data distribution, making predictions more robust, \\nespecially for underrepresented classes.\\n\\nFeature Engineering: Identifying and engineering additional relevant features may improve the model’s ability to distinguish between positive and negative cases. \\nFor instance, interaction terms between age, bmi, and hypertension may provide more insight into the factors leading to stroke.\\n\\nAlternative Architectures: Exploring architectures other than Transformers, such as convolutional neural networks (CNNs) for tabular data or recurrent neural networks (RNNs) for sequential dependencies, \\ncould yield better results depending on the data\\'s nature.\\n\\nFinal Remarks\\nThe Transformer-based model provides a strong starting point, particularly in capturing complex patterns among the features. \\nHowever, due to the dataset’s class imbalance, the model currently lacks sufficient sensitivity to detect positive stroke cases reliably. \\nBy addressing the issues highlighted—especially class balancing and threshold tuning—future iterations of the model can improve recall for positive cases, \\nmaking it more practical for real-world applications in healthcare settings. \\nThe insights gathered from this model can serve as a valuable foundation for developing a reliable stroke prediction system to assist medical professionals in early diagnosis and intervention.\\n\\n'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "Machine Learning Project Report: Stroke Prediction Model Using Transformer-Based Binary Classifier\n",
    "Project Objective\n",
    "The objective of this project is to build a machine learning model to predict the likelihood of a stroke in a given patient based on medical and demographic features. A Transformer-based binary classifier was implemented and trained on a dataset of patient data to classify individuals into \"stroke\" or \"no stroke\" categories.\n",
    "\n",
    "Dataset Description\n",
    "The dataset contains patient demographic and health-related attributes relevant to stroke prediction, including:\n",
    "\n",
    "Patient ID: Unique identifier for each patient.\n",
    "Gender: Patient gender (e.g., Male, Female).\n",
    "Age: Patient's age.\n",
    "Hypertension: Presence of hypertension (1 for yes, 0 for no).\n",
    "Heart Disease: Presence of heart disease (1 for yes, 0 for no).\n",
    "Ever Married: Marital status (Yes/No).\n",
    "Work Type: Employment type (e.g., Private, Self-employed).\n",
    "Residence Type: Rural or Urban.\n",
    "Average Glucose Level: Average glucose level in blood.\n",
    "BMI: Body Mass Index.\n",
    "Smoking Status: Smoking behavior (e.g., never smoked, smokes).\n",
    "Stroke (Target): Binary label indicating if the patient had a stroke (1 for yes, 0 for no).\n",
    "Data Preprocessing\n",
    "To prepare the dataset for training, the following steps were undertaken:\n",
    "\n",
    "Handling Missing Values:\n",
    "\n",
    "Missing values in the BMI column were replaced with the column's mean.\n",
    "Encoding Categorical Features:\n",
    "\n",
    "Categorical columns such as gender, ever_married, work_type, Residence_type, and smoking_status were encoded using label encoding.\n",
    "Scaling Numerical Features:\n",
    "\n",
    "Numerical columns (age, avg_glucose_level, and bmi) were standardized to have zero mean and unit variance.\n",
    "Feature-Target Split:\n",
    "\n",
    "The id column was dropped, and the remaining features were used to create the feature matrix (X) and target vector (y).\n",
    "Train-Test Split:\n",
    "\n",
    "The dataset was split into training (80%) and testing (20%) sets to evaluate the model's generalization performance.\n",
    "Model Architecture\n",
    "The binary classifier utilized a Transformer architecture designed to capture complex interactions among input features. The architecture included the following components:\n",
    "\n",
    "Input Embedding:\n",
    "\n",
    "A linear layer projected input features into a 256-dimensional embedding space.\n",
    "Transformer Encoder:\n",
    "\n",
    "The model consisted of 8 Transformer encoder layers, each with:\n",
    "256-dimensional embeddings.\n",
    "8 attention heads.\n",
    "Feedforward networks with ReLU activations.\n",
    "Residual connections and layer normalization for improved gradient flow.\n",
    "Global Pooling and Output:\n",
    "\n",
    "Global average pooling aggregated the sequence of feature embeddings.\n",
    "A fully connected output layer mapped the pooled representation to a single logit for binary classification.\n",
    "Loss Function:\n",
    "\n",
    "Binary Cross-Entropy with Logits Loss (BCEWithLogitsLoss) was used, incorporating class weighting to address the imbalance in the dataset.\n",
    "Model Training\n",
    "The model was trained using the following hyperparameters:\n",
    "\n",
    "Optimizer: Adam optimizer.\n",
    "Learning Rate: 0.0001 with StepLR scheduling (decayed by 0.1 every 5 epochs).\n",
    "Batch Size: 32.\n",
    "Epochs: 20.\n",
    "Dropout Rate: 0.1 to prevent overfitting.\n",
    "Model Evaluation\n",
    "Binary Classification Metrics at Optimal Threshold:\n",
    "Optimal Threshold: 0.9778\n",
    "Accuracy: 93.84%\n",
    "Precision: 30.23%\n",
    "Recall: 28.26%\n",
    "F1 Score: 29.21%\n",
    "AUC-ROC: 85.19%\n",
    "AUC-PR: 20.66%\n",
    "Observations\n",
    "Strengths:\n",
    "\n",
    "The model achieved high accuracy (93.84%), indicating its ability to correctly classify most samples, especially for the majority class (no stroke).\n",
    "AUC-ROC (85.19%) suggests strong discriminative ability in distinguishing between stroke and non-stroke cases.\n",
    "Weaknesses:\n",
    "\n",
    "Precision (30.23%) and recall (28.26%) remained low, indicating difficulties in identifying actual stroke cases.\n",
    "The low F1 score (29.21%) reflects an imbalance between precision and recall, particularly for the minority class (stroke).\n",
    "Insights from AUC-PR:\n",
    "\n",
    "The AUC-PR score of 20.66% highlights challenges in achieving high precision and recall for positive cases, a common issue in imbalanced datasets.\n",
    "Conclusion\n",
    "The Transformer-based binary classifier performed well in terms of overall accuracy and AUC-ROC but struggled with recall and F1 score, indicating difficulty in detecting positive stroke cases. The high accuracy score reflects the model's tendency to favor the majority class, which is typical in imbalanced datasets.\n",
    "\n",
    "Future Improvements\n",
    "Class Balancing:\n",
    "\n",
    "Use advanced oversampling techniques such as SMOTE or under-sampling of the majority class.\n",
    "Threshold Tuning:\n",
    "\n",
    "Experiment with alternative thresholds to optimize recall for positive cases.\n",
    "Alternative Loss Functions:\n",
    "\n",
    "Replace BCEWithLogitsLoss with focal loss to penalize false negatives more heavily.\n",
    "Feature Engineering:\n",
    "\n",
    "Include engineered features such as interaction terms (e.g., age × BMI) to enhance predictive performance.\n",
    "Hyperparameter Optimization:\n",
    "\n",
    "Further tune embedding size, the number of encoder layers, and attention heads.\n",
    "Comparative Analysis:\n",
    "\n",
    "Evaluate simpler models such as logistic regression or decision trees as baselines to validate the complexity of the Transformer-based approach.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ab24d51-c6a7-4869-9159-956c33636c77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
