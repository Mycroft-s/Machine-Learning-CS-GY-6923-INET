{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cdfb0dc-5e71-41b4-ba5a-35ff72991191",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in each column:\n",
      " id                     0\n",
      "gender                 0\n",
      "age                    0\n",
      "hypertension           0\n",
      "heart_disease          0\n",
      "ever_married           0\n",
      "work_type              0\n",
      "Residence_type         0\n",
      "avg_glucose_level      0\n",
      "bmi                  201\n",
      "smoking_status         0\n",
      "stroke                 0\n",
      "dtype: int64\n",
      "Training indices: Index([4965, 2070, 4604, 4524, 1742, 2206, 3538, 1446, 1383, 2446], dtype='int64')\n",
      "Testing indices: Index([860, 2153, 864, 4494, 4205, 4210, 5067, 2977, 1540, 2840], dtype='int64')\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load dataset\n",
    "data_path = r'C:\\Users\\Admin\\Desktop\\ML online\\final project\\healthcare-dataset-stroke-data.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Check missing values\n",
    "print(\"Missing values in each column:\\n\", data.isnull().sum())\n",
    "\n",
    "# Fill missing values in 'bmi' column\n",
    "data['bmi'] = data['bmi'].fillna(data['bmi'].mean())\n",
    "\n",
    "# Encode categorical features\n",
    "label_encoders = {}\n",
    "for column in ['gender', 'ever_married', 'work_type', 'Residence_type', 'smoking_status']:\n",
    "    label_encoders[column] = LabelEncoder()\n",
    "    data[column] = label_encoders[column].fit_transform(data[column])\n",
    "\n",
    "# Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "data[['age', 'avg_glucose_level', 'bmi']] = scaler.fit_transform(data[['age', 'avg_glucose_level', 'bmi']])\n",
    "\n",
    "# Split features and target\n",
    "X = data.drop(columns=['id', 'stroke'])\n",
    "y = data['stroke']\n",
    "\n",
    "# Split dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "print(\"Training indices:\", X_train.index[:10])\n",
    "print(\"Testing indices:\", X_test.index[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "d819d7cd-625b-4c79-bae6-dab7dd9b20ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.2032\n",
      "Epoch 2, Loss: 0.1922\n",
      "Epoch 3, Loss: 0.2039\n",
      "Epoch 4, Loss: 0.2016\n",
      "Epoch 5, Loss: 0.2019\n",
      "Epoch 6, Loss: 0.1953\n",
      "Epoch 7, Loss: 0.2004\n",
      "Epoch 8, Loss: 0.2003\n",
      "Epoch 9, Loss: 0.2011\n",
      "Epoch 10, Loss: 0.2008\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define Transformer-based binary classifier\n",
    "class TransformerBinaryClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(TransformerBinaryClassifier, self).__init__()\n",
    "        self.embedding = nn.Linear(input_dim, 64)  # Embedding layer to transform input to d_model size\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=64, nhead=8, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=4)\n",
    "        self.fc = nn.Linear(64, 1)  # Output one value for binary classification\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Ensure x is a tensor before proceeding\n",
    "        x = torch.tensor(x, dtype=torch.float32) if not isinstance(x, torch.Tensor) else x\n",
    "        x = self.embedding(x).unsqueeze(1)  # Transform input to shape (batch_size, 1, d_model)\n",
    "        x = self.transformer_encoder(x)  # Output shape: (batch_size, 1, d_model)\n",
    "        x = x.mean(dim=1)  # Apply mean pooling to get shape (batch_size, d_model)\n",
    "        return self.fc(x)  # Output shape: (batch_size, 1)\n",
    "\n",
    "# Initialize model, loss function, and optimizer\n",
    "input_dim = X_train.shape[1]\n",
    "model = TransformerBinaryClassifier(input_dim=input_dim)\n",
    "\n",
    "# Use BCEWithLogitsLoss for binary classification\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Prepare data loaders\n",
    "train_dataset = TensorDataset(torch.tensor(X_train.values, dtype=torch.float32), torch.tensor(y_train.values, dtype=torch.float32))\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Define training function for binary classification\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=10):\n",
    "    model.train()\n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs).squeeze(1)  # Shape: (batch_size,)\n",
    "            loss = criterion(outputs, labels)  # Binary cross-entropy loss\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "5f2877ca-c804-4ca8-a923-78299fb59273",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.040117416829745595\n",
      "Precision: 0.040117416829745595\n",
      "Recall: 1.0\n",
      "F1 Score: 0.07714016933207903\n",
      "Average Precision: 0.05565009331542165\n",
      "Error calculating Hamming Loss: Classification metrics can't handle a mix of multilabel-indicator and binary targets\n",
      "One Error: 0.040117416829745595\n",
      "Ranking Loss: 0.040117416829745595\n",
      "Coverage: -0.9598825831702544\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, average_precision_score, hamming_loss, label_ranking_loss\n",
    "\n",
    "# 确保 y_test 是一个二维矩阵，适用于二分类任务\n",
    "if y_test.ndim == 1:\n",
    "    print(\"Reshaping y_test from 1D to 2D for binary classification.\")\n",
    "    y_test = y_test.values.reshape(-1, 1)\n",
    "\n",
    "# 准备测试数据加载器\n",
    "test_dataset = TensorDataset(torch.tensor(X_test.values, dtype=torch.float32), torch.tensor(y_test, dtype=torch.float32))\n",
    "test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "# 评估模型\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "# 使用测试集生成预测值\n",
    "with torch.no_grad():\n",
    "    for inputs, labels in test_loader:\n",
    "        outputs = model(inputs)\n",
    "        probs = torch.sigmoid(outputs)  # 转换为概率\n",
    "        all_preds.append(probs)\n",
    "        all_labels.append(labels)\n",
    "\n",
    "# 将预测值和真实标签转换为 NumPy 数组\n",
    "all_preds = torch.cat(all_preds).cpu().numpy()\n",
    "all_labels = torch.cat(all_labels).cpu().numpy()\n",
    "\n",
    "# 调整阈值并将概率转换为二进制预测\n",
    "threshold = 0.01\n",
    "binarized_preds = (all_preds >= threshold).astype(int)\n",
    "\n",
    "# 计算二分类指标\n",
    "try:\n",
    "    accuracy = accuracy_score(all_labels, binarized_preds)\n",
    "    precision = precision_score(all_labels, binarized_preds, zero_division=1)\n",
    "    recall = recall_score(all_labels, binarized_preds)\n",
    "    f1 = f1_score(all_labels, binarized_preds)\n",
    "    average_precision = average_precision_score(all_labels, all_preds)\n",
    "\n",
    "    print(\"Accuracy:\", accuracy)\n",
    "    print(\"Precision:\", precision)\n",
    "    print(\"Recall:\", recall)\n",
    "    print(\"F1 Score:\", f1)\n",
    "    print(\"Average Precision:\", average_precision)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating binary classification metrics:\", e)\n",
    "\n",
    "# 将预测值转换为多标签格式\n",
    "all_preds_multilabel = np.hstack([(1 - all_preds), all_preds])  # (n_samples, 2)\n",
    "all_labels_multilabel = np.hstack([(1 - all_labels), all_labels])  # (n_samples, 2)\n",
    "\n",
    "# 计算多标签评估指标\n",
    "\n",
    "# Hamming Loss - 使用多标签格式\n",
    "try:\n",
    "    hamming_loss_score = hamming_loss(all_labels_multilabel, binarized_preds)\n",
    "    print(\"Hamming Loss:\", hamming_loss_score)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating Hamming Loss:\", e)\n",
    "\n",
    "# One Error\n",
    "def one_error(y_true, y_pred):\n",
    "    one_error_count = 0\n",
    "    for idx, (true_labels, pred_scores) in enumerate(zip(y_true, y_pred)):\n",
    "        top_pred_idx = np.argmax(pred_scores)\n",
    "        if true_labels[top_pred_idx] == 0:\n",
    "            one_error_count += 1\n",
    "    return one_error_count / len(y_true)\n",
    "\n",
    "try:\n",
    "    one_error_score = one_error(all_labels_multilabel, all_preds_multilabel)\n",
    "    print(\"One Error:\", one_error_score)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating One Error:\", e)\n",
    "\n",
    "# Ranking Loss\n",
    "try:\n",
    "    ranking_loss_score = label_ranking_loss(all_labels_multilabel, all_preds_multilabel)\n",
    "    print(\"Ranking Loss:\", ranking_loss_score)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating Ranking Loss:\", e)\n",
    "\n",
    "# Coverage\n",
    "def coverage(y_true, y_pred):\n",
    "    coverage_sum = 0\n",
    "    for idx, (true_labels, pred_scores) in enumerate(zip(y_true, y_pred)):\n",
    "        relevant_indices = np.where(true_labels == 1)[0]\n",
    "        if len(relevant_indices) == 0:\n",
    "            continue  # 跳过没有相关标签的样本\n",
    "        sorted_indices = np.argsort(-pred_scores)\n",
    "        max_rank = max(np.where(sorted_indices == idx)[0][0] for idx in relevant_indices)\n",
    "        coverage_sum += max_rank\n",
    "    return (coverage_sum / len(y_true)) - 1\n",
    "\n",
    "try:\n",
    "    coverage_score = coverage(all_labels_multilabel, all_preds_multilabel)\n",
    "    print(\"Coverage:\", coverage_score)\n",
    "except Exception as e:\n",
    "    print(\"Error calculating Coverage:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d19e58a7-ebfb-49a4-bd32-87a52461ac94",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Machine Learning Project Report: Stroke Prediction Model Using Transformer-Based Binary Classifier\n",
    "Project Objective\n",
    "The objective of this project is to build a machine learning model to predict the likelihood of a stroke in a given patient based on a set of medical and demographic features. \n",
    "We use a Transformer-based binary classifier, trained on a dataset of patient data, to classify each patient into \"stroke\" or \"no stroke\" categories.\n",
    "\n",
    "\n",
    "Dataset Description\n",
    "The dataset contains information on patient demographics and health metrics relevant to stroke prediction. \n",
    "The features include:\n",
    "\n",
    "Patient ID: Unique identifier for each patient.\n",
    "Gender: Patient gender (e.g., Male, Female).\n",
    "Age: Patient age.\n",
    "Hypertension: Whether the patient has hypertension (1) or not (0).\n",
    "Heart Disease: Whether the patient has any heart disease (1) or not (0).\n",
    "Ever Married: Whether the patient has ever been married.\n",
    "Work Type: Employment type (e.g., Private, Self-employed).\n",
    "Residence Type: Residence location (Rural or Urban).\n",
    "Average Glucose Level: Average glucose level in the blood.\n",
    "BMI: Body Mass Index of the patient.\n",
    "Smoking Status: Smoking behavior (e.g., never smoked, smokes).\n",
    "Stroke (target): Whether the patient had a stroke (1) or not (0).\n",
    "\n",
    "\n",
    "Data Preprocessing\n",
    "Data preprocessing steps include handling missing values, encoding categorical features, \n",
    "and scaling numerical features to prepare the data for training.\n",
    "\n",
    "Handling Missing Values: \n",
    "We identified and filled missing values in the 'BMI' column with the column mean.\n",
    "Encoding Categorical Features: \n",
    "Label encoding was applied to categorical columns such as 'gender', 'ever_married', 'work_type', 'Residence_type', and 'smoking_status'.\n",
    "Scaling Numerical Features: \n",
    "Standard scaling was applied to numerical columns (age, avg_glucose_level, and bmi) to normalize the data and enhance the model's learning stability.\n",
    "Feature-Target Split: \n",
    "We dropped the 'id' and 'stroke' columns to create feature matrix X and target vector y. The 'stroke' column serves as the target variable.\n",
    "\n",
    "Train-Test Split\n",
    "To evaluate the model’s performance, the dataset was split into training and testing sets with a ratio of 80:20. This ensures that 80% of the data is used for training, and 20% is held back for evaluation.\n",
    "\n",
    "\n",
    "Model Architecture\n",
    "We implemented a binary classifier based on a Transformer architecture to capture complex patterns in the data.\n",
    "\n",
    "Embedding Layer: The input features are transformed into a higher-dimensional representation through an embedding layer (nn.Linear(input_dim, 64)), \n",
    "with 64 being the transformer’s model dimension.\n",
    "Transformer Encoder: We used four layers of Transformer encoders with eight attention heads each. \n",
    "This encoder learns contextual relationships between features.\n",
    "Fully Connected Layer: The final layer (nn.Linear(64, 1)) outputs a single logit, \n",
    "which is then passed through a sigmoid function during evaluation to represent the probability of a stroke.\n",
    "Binary Cross-Entropy Loss: Since this is a binary classification problem, we used BCEWithLogitsLoss, \n",
    "which combines the sigmoid activation and binary cross-entropy loss functions, \n",
    "allowing us to work directly with logits for increased numerical stability.\n",
    "Model Training\n",
    "The model was trained using the Adam optimizer, a batch size of 32, and a learning rate of 0.001 over 10 epochs. \n",
    "During each epoch, the model processed batches of input data, computed the loss, and adjusted the weights accordingly to minimize the binary cross-entropy loss.\n",
    "\n",
    "Model Evaluation\n",
    "We evaluated the model using both binary and multi-label metrics to assess performance. These included:\n",
    "\n",
    "Binary Classification Metrics:\n",
    "\n",
    "Accuracy: Overall correctness of predictions.\n",
    "Precision: Correctness of positive predictions.\n",
    "Recall: Sensitivity of the model to actual positive cases.\n",
    "F1 Score: Harmonic mean of precision and recall.\n",
    "Average Precision: Area under the precision-recall curve, capturing the model’s ability to predict positive instances accurately.\n",
    "Multi-label Metrics (simulated for binary classification):\n",
    "\n",
    "Hamming Loss: Measures the fraction of incorrect predictions in the binary format.\n",
    "One Error: Checks if the highest-ranked prediction is an actual positive.\n",
    "Ranking Loss: Measures if true labels are ranked above false labels.\n",
    "Coverage: Measures the range needed to cover all true labels in the ranked predictions.\n",
    "\n",
    "Observations from Model Evaluation\n",
    "Binary Metrics:\n",
    "The model achieved a high accuracy score, indicating it correctly identified the majority of cases. \n",
    "However, precision and recall were imbalanced, likely due to the presence of fewer positive cases (stroke instances) in the dataset.\n",
    "Recall and F1 Score were low, suggesting the model struggles to detect true positive cases (i.e., stroke cases).\n",
    "Multi-label Metrics:\n",
    "Hamming Loss was significant, indicating a fair amount of prediction error in binary classifications.\n",
    "One Error and Ranking Loss yielded reasonable scores, suggesting the model ranked relevant labels appropriately when present.\n",
    "Coverage was negative, possibly due to sparse true positive cases.\n",
    "\n",
    "\n",
    "Conclusion\n",
    "The Transformer-based model demonstrated good performance on the negative cases but struggled with positive cases, \n",
    "which is evident from the low recall score. This outcome indicates that the model is likely biased toward predicting the majority class. This issue is common in imbalanced datasets, where the model learns to prioritize the majority class. Possible next steps for improvement include:\n",
    "\n",
    "Class Balancing: Apply techniques such as class weighting or oversampling of the minority class to improve model sensitivity to positive cases.\n",
    "Threshold Adjustment: Experiment with different thresholds to find the optimal decision boundary for stroke prediction.\n",
    "Simpler Models for Comparison: Testing simpler models, such as logistic regression or decision trees, \n",
    "could provide a useful baseline to compare the performance of the Transformer-based model. \n",
    "In some cases, simpler models can perform as well as or better than complex models, \n",
    "especially when the dataset is limited in size or heavily imbalanced.\n",
    "\n",
    "Further Hyperparameter Tuning: Experimenting with hyperparameters such as the number of Transformer encoder layers, \n",
    "the embedding size, and the number of attention heads may lead to performance improvements. Additionally, adjusting the learning rate, \n",
    "batch size, or the number of epochs could help optimize the model’s learning process.\n",
    "\n",
    "Ensemble Methods: Implementing an ensemble of models (e.g., combining Transformer with simpler models) \n",
    "could potentially enhance performance by capturing different aspects of the data distribution, making predictions more robust, \n",
    "especially for underrepresented classes.\n",
    "\n",
    "Feature Engineering: Identifying and engineering additional relevant features may improve the model’s ability to distinguish between positive and negative cases. \n",
    "For instance, interaction terms between age, bmi, and hypertension may provide more insight into the factors leading to stroke.\n",
    "\n",
    "Alternative Architectures: Exploring architectures other than Transformers, such as convolutional neural networks (CNNs) for tabular data or recurrent neural networks (RNNs) for sequential dependencies, \n",
    "could yield better results depending on the data's nature.\n",
    "\n",
    "Final Remarks\n",
    "The Transformer-based model provides a strong starting point, particularly in capturing complex patterns among the features. \n",
    "However, due to the dataset’s class imbalance, the model currently lacks sufficient sensitivity to detect positive stroke cases reliably. \n",
    "By addressing the issues highlighted—especially class balancing and threshold tuning—future iterations of the model can improve recall for positive cases, \n",
    "making it more practical for real-world applications in healthcare settings. \n",
    "The insights gathered from this model can serve as a valuable foundation for developing a reliable stroke prediction system to assist medical professionals in early diagnosis and intervention.\n",
    "\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
