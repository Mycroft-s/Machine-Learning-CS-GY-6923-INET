{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c988c24f-913c-4540-84c4-67dc5e4d4555",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载 Iris 数据集\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data  # 特征\n",
    "y = iris.target  # 标签\n",
    "\n",
    "from sklearn.metrics import accuracy_score, recall_score, f1_score, precision_score, hamming_loss, confusion_matrix\n",
    "\n",
    "def evaluate_model(y_true, y_pred):\n",
    "    # Accuracy\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    \n",
    "    # Precision (Macro-average: treats all classes equally)\n",
    "    precision = precision_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    \n",
    "    # Recall (Macro-average: treats all classes equally)\n",
    "    recall = recall_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    \n",
    "    # F1 Score (Macro-average: treats all classes equally)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro', zero_division=1)\n",
    "    \n",
    "    # Hamming Loss\n",
    "    h_loss = hamming_loss(y_true, y_pred)\n",
    "    \n",
    "    # Confusion Matrix\n",
    "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Print results\n",
    "    print(f'Accuracy: {acc:.2f}')\n",
    "    print(f'Precision (macro): {precision:.2f}')\n",
    "    print(f'Recall (macro): {recall:.2f}')\n",
    "    print(f'F1 Score (macro): {f1:.2f}')\n",
    "    print(f'Hamming Loss: {h_loss:.2f}')\n",
    "    print('Confusion Matrix:')\n",
    "    print(conf_matrix)\n",
    "    \n",
    "    return acc, precision, recall, f1, h_loss, conf_matrix\n",
    "\n",
    "# 标准化函数\n",
    "def standardize_data(X_train, X_test):\n",
    "    sc = StandardScaler()\n",
    "    X_train_std = sc.fit_transform(X_train)\n",
    "    X_test_std = sc.transform(X_test)\n",
    "    return X_train_std, X_test_std\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae45836c-6f24-4eda-9314-0455bcc38c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (2 features, linear):\n",
      "Accuracy: 0.53\n",
      "Precision (macro): 0.67\n",
      "Recall (macro): 0.67\n",
      "F1 Score (macro): 0.33\n",
      "Hamming Loss: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [14  0  0]\n",
      " [ 0  0 16]]\n",
      "Adaline Results (2 features, linear):\n",
      "Accuracy: 0.53\n",
      "Precision (macro): 0.55\n",
      "Recall (macro): 0.67\n",
      "F1 Score (macro): 0.26\n",
      "Hamming Loss: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 5  0  9]\n",
      " [ 0  0 16]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5333333333333333,\n",
       " np.float64(0.5466666666666667),\n",
       " np.float64(0.6666666666666666),\n",
       " np.float64(0.2601626016260163),\n",
       " 0.4666666666666667,\n",
       " array([[ 0,  0,  0],\n",
       "        [ 5,  0,  9],\n",
       "        [ 0,  0, 16]]))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear 2 class 2 feature \n",
    "X_2_features = X[y != 2, :2]  # 选择前两个特征 feature\n",
    "y_2_features = y[y != 2]      # 选择类别0和1   label (0：Iris-setosa； 1：Iris-versicolor)\n",
    "\n",
    "# 分割数据集为训练集和测试集   Split the data set into a training set and a test set\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2_features, y_2_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据 standardized data\n",
    "X_train_2_std, X_test_2_std = standardize_data(X_train_2, X_test_2)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ppn_2 = ppn.predict(X_test_2_std)\n",
    "\n",
    "print('Perceptron Results (2 features, linear):')\n",
    "evaluate_model(y_test_2, y_pred_ppn_2)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada = AdalineGD(eta=0.01, n_iter=50)\n",
    "ada.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ada_2 = ada.predict(X_test_2_std)\n",
    "\n",
    "print('Adaline Results (2 features, linear):')\n",
    "evaluate_model(y_test_2, y_pred_ada_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b6983ca6-9d4b-41a9-99d4-2048c19a548a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (3 features, linear):\n",
      "Accuracy: 0.53\n",
      "Precision (macro): 0.67\n",
      "Recall (macro): 0.67\n",
      "F1 Score (macro): 0.33\n",
      "Hamming Loss: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [14  0  0]\n",
      " [ 0  0 16]]\n",
      "Adaline Results (3 features, linear):\n",
      "Accuracy: 0.53\n",
      "Precision (macro): 0.57\n",
      "Recall (macro): 0.67\n",
      "F1 Score (macro): 0.27\n",
      "Hamming Loss: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 7  0  7]\n",
      " [ 0  0 16]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.5333333333333333,\n",
       " np.float64(0.5652173913043478),\n",
       " np.float64(0.6666666666666666),\n",
       " np.float64(0.2735042735042735),\n",
       " 0.4666666666666667,\n",
       " array([[ 0,  0,  0],\n",
       "        [ 7,  0,  7],\n",
       "        [ 0,  0, 16]]))"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 选2 class 3 feature \n",
    "X_3_features = X[y != 2, :3]  # 选择前三个特征 feature\n",
    "y_3_features = y[y != 2]      # 选择类别0和1  (0：Iris-setosa； 1：Iris-versicolor)\n",
    "\n",
    "# 分割数据集为训练集和测试集 Split the data set into a training set and a test set\n",
    "X_train_3, X_test_3, y_train_3, y_test_3 = train_test_split(X_3_features, y_3_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据  standardized data\n",
    "X_train_3_std, X_test_3_std = standardize_data(X_train_3, X_test_3)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn.fit(X_train_3_std, y_train_3)\n",
    "y_pred_ppn_3 = ppn.predict(X_test_3_std)\n",
    "\n",
    "print('Perceptron Results (3 features, linear):')\n",
    "evaluate_model(y_test_3, y_pred_ppn_3)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada.fit(X_train_3_std, y_train_3)\n",
    "y_pred_ada_3 = ada.predict(X_test_3_std)\n",
    "\n",
    "print('Adaline Results (3 features, linear):')\n",
    "evaluate_model(y_test_3, y_pred_ada_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "18bd537a-60e3-4b91-a122-47e528572705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (4 features, linear):\n",
      "Accuracy: 0.53\n",
      "Precision (macro): 0.77\n",
      "Recall (macro): 0.50\n",
      "F1 Score (macro): 0.35\n",
      "Hamming Loss: 0.47\n",
      "Confusion Matrix:\n",
      "[[ 0 14]\n",
      " [ 0 16]]\n",
      "Adaline Results (4 features, linear):\n",
      "Accuracy: 0.00\n",
      "Precision (macro): 0.33\n",
      "Recall (macro): 0.33\n",
      "F1 Score (macro): 0.00\n",
      "Hamming Loss: 1.00\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 0  0 14]\n",
      " [16  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.0,\n",
       " np.float64(0.3333333333333333),\n",
       " np.float64(0.3333333333333333),\n",
       " np.float64(0.0),\n",
       " 1.0,\n",
       " array([[ 0,  0,  0],\n",
       "        [ 0,  0, 14],\n",
       "        [16,  0,  0]]))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2 class 4 feature   \n",
    "X_4_features = X[y != 2, :]  # 选择所有四个特征  4feature\n",
    "y_4_features = y[y != 2]     # 选择类别0和1  label  (0：Iris-setosa； 1：Iris-versicolor)\n",
    "\n",
    "# 分割数据集为训练集和测试集\n",
    "X_train_4, X_test_4, y_train_4, y_test_4 = train_test_split(X_4_features, y_4_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据\n",
    "X_train_4_std, X_test_4_std = standardize_data(X_train_4, X_test_4)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn.fit(X_train_4_std, y_train_4)\n",
    "y_pred_ppn_4 = ppn.predict(X_test_4_std)\n",
    "\n",
    "print('Perceptron Results (4 features, linear):')\n",
    "evaluate_model(y_test_4, y_pred_ppn_4)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada.fit(X_train_4_std, y_train_4)\n",
    "y_pred_ada_4 = ada.predict(X_test_4_std)\n",
    "\n",
    "print('Adaline Results (4 features, linear):')\n",
    "evaluate_model(y_test_4, y_pred_ada_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "42bbe41c-5791-485f-a1d7-f4defbb396d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# not linear (Iris-versicolor (1); Iris-virginica (2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "97ba76d4-b305-43f3-baf6-664e3a951058",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (2 features, not linear):\n",
      "Accuracy: 0.43\n",
      "Precision (macro): 0.48\n",
      "Recall (macro): 0.64\n",
      "F1 Score (macro): 0.20\n",
      "Hamming Loss: 0.57\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 1 13  0]\n",
      " [ 0 16  0]]\n",
      "Adaline Results (2 features, not linear):\n",
      "Accuracy: 0.47\n",
      "Precision (macro): 0.73\n",
      "Recall (macro): 0.50\n",
      "F1 Score (macro): 0.32\n",
      "Hamming Loss: 0.53\n",
      "Confusion Matrix:\n",
      "[[14  0]\n",
      " [16  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4666666666666667,\n",
       " np.float64(0.7333333333333334),\n",
       " np.float64(0.5),\n",
       " np.float64(0.3181818181818182),\n",
       " 0.5333333333333333,\n",
       " array([[14,  0],\n",
       "        [16,  0]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear 2 class 2 feature \n",
    "X_2_features = X[y != 2, :2]  # 选择前两个特征 feature\n",
    "y_2_features = y[y != 0]      # 选择类别1和2   label (Iris-versicolor (1); Iris-virginica (2))\n",
    "\n",
    "# 分割数据集为训练集和测试集   Split the data set into a training set and a test set\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2_features, y_2_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据 standardized data\n",
    "X_train_2_std, X_test_2_std = standardize_data(X_train_2, X_test_2)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ppn_2 = ppn.predict(X_test_2_std)\n",
    "\n",
    "print('Perceptron Results (2 features, not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ppn_2)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada = AdalineGD(eta=0.01, n_iter=50)\n",
    "ada.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ada_2 = ada.predict(X_test_2_std)\n",
    "\n",
    "print('Adaline Results (2 features, not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ada_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "184bcf97-e8e6-43b2-9888-a4ae77edcff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (3 features,  not linear):\n",
      "Accuracy: 0.40\n",
      "Precision (macro): 0.48\n",
      "Recall (macro): 0.62\n",
      "F1 Score (macro): 0.19\n",
      "Hamming Loss: 0.60\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 2 12  0]\n",
      " [ 0 16  0]]\n",
      "Adaline Results (3 features, not linear):\n",
      "Accuracy: 0.47\n",
      "Precision (macro): 0.73\n",
      "Recall (macro): 0.50\n",
      "F1 Score (macro): 0.32\n",
      "Hamming Loss: 0.53\n",
      "Confusion Matrix:\n",
      "[[14  0]\n",
      " [16  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4666666666666667,\n",
       " np.float64(0.7333333333333334),\n",
       " np.float64(0.5),\n",
       " np.float64(0.3181818181818182),\n",
       " 0.5333333333333333,\n",
       " array([[14,  0],\n",
       "        [16,  0]]))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear 2 class 3 feature \n",
    "X_2_features = X[y != 2, :3]  # 选择前3个特征 feature\n",
    "y_2_features = y[y != 0]      # 选择类别1和2   label (Iris-versicolor (1); Iris-virginica (2))\n",
    "\n",
    "# 分割数据集为训练集和测试集   Split the data set into a training set and a test set\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2_features, y_2_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据 standardized data\n",
    "X_train_2_std, X_test_2_std = standardize_data(X_train_2, X_test_2)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ppn_2 = ppn.predict(X_test_2_std)\n",
    "\n",
    "print('Perceptron Results (3 features,  not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ppn_2)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada = AdalineGD(eta=0.01, n_iter=50)\n",
    "ada.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ada_2 = ada.predict(X_test_2_std)\n",
    "\n",
    "print('Adaline Results (3 features, not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ada_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "041fe466-7e0f-4123-9d51-f4736d774ad5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perceptron Results (4 features, not linear):\n",
      "Accuracy: 0.37\n",
      "Precision (macro): 0.47\n",
      "Recall (macro): 0.60\n",
      "F1 Score (macro): 0.18\n",
      "Hamming Loss: 0.63\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 3 11  0]\n",
      " [ 0 16  0]]\n",
      "Adaline Results (4 features, not linear):\n",
      "Accuracy: 0.47\n",
      "Precision (macro): 0.67\n",
      "Recall (macro): 0.67\n",
      "F1 Score (macro): 0.33\n",
      "Hamming Loss: 0.53\n",
      "Confusion Matrix:\n",
      "[[ 0  0  0]\n",
      " [ 0 14  0]\n",
      " [16  0  0]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4666666666666667,\n",
       " np.float64(0.6666666666666666),\n",
       " np.float64(0.6666666666666666),\n",
       " np.float64(0.3333333333333333),\n",
       " 0.5333333333333333,\n",
       " array([[ 0,  0,  0],\n",
       "        [ 0, 14,  0],\n",
       "        [16,  0,  0]]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# linear 2 class 4 feature \n",
    "X_2_features = X[y != 2, :]  # 选择前4个特征 feature\n",
    "y_2_features = y[y != 0]      # 选择类别1和2   label (Iris-versicolor (1); Iris-virginica (2))\n",
    "\n",
    "# 分割数据集为训练集和测试集   Split the data set into a training set and a test set\n",
    "X_train_2, X_test_2, y_train_2, y_test_2 = train_test_split(X_2_features, y_2_features, test_size=0.3, random_state=1)\n",
    "\n",
    "# 标准化数据 standardized data\n",
    "X_train_2_std, X_test_2_std = standardize_data(X_train_2, X_test_2)\n",
    "\n",
    "# 训练并评估 Perceptron 模型\n",
    "ppn = Perceptron(eta=0.1, n_iter=10)\n",
    "ppn.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ppn_2 = ppn.predict(X_test_2_std)\n",
    "\n",
    "print('Perceptron Results (4 features, not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ppn_2)\n",
    "\n",
    "# 训练并评估 Adaline 模型\n",
    "ada = AdalineGD(eta=0.01, n_iter=50)\n",
    "ada.fit(X_train_2_std, y_train_2)\n",
    "y_pred_ada_2 = ada.predict(X_test_2_std)\n",
    "\n",
    "print('Adaline Results (4 features, not linear):')\n",
    "evaluate_model(y_test_2, y_pred_ada_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b710e076-149f-45c2-b785-192542cd3270",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nexperiment result and analyze:\\n1. Linear Classification (Linearly Separable Data)\\n1.1 Perceptron vs Adaline with 2 Features\\nIn the case of two linearly separable features, both models performed similarly in terms of accuracy. \\nHowever, Perceptron had a slight edge in precision and F1 score, while Adaline was better at handling class imbalance. \\nPerceptron failed to correctly classify any of the samples from class 1, whereas Adaline managed to classify a portion of class 1 correctly, suggesting that Adaline is slightly more robust in handling imbalanced data.\\n\\n1.2 Perceptron vs Adaline with 3 Features\\nWith three features, the performance of both models remained quite similar to the two-feature case. \\nAdaline continued to perform slightly better in handling class imbalance by correctly classifying some class 1 samples. \\nPerceptron maintained its accuracy but still struggled with class 1 classification. \\nThis suggests that Adaline shows some degree of flexibility in handling additional features in linearly separable conditions.\\n\\n1.3 Perceptron vs Adaline with 4 Features\\nWhen testing with all four linearly separable features, Perceptron managed to maintain a relatively stable performance, although its recall for class 1 was quite low, indicating that it struggled to classify class 1 correctly. \\nIn contrast, Adaline performed extremely poorly in this scenario, failing to make any correct classifications. This suggests that as the feature space becomes more complex, Perceptron is able to maintain its performance, while Adaline is less capable of handling more dimensions, leading to a complete breakdown in performance.\\n\\n2. Non-Linear Classification (Non-Linearly Separable Data)\\n2.1 Perceptron vs Adaline with 2 Features\\nWhen dealing with non-linearly separable data with two features, both models saw a decline in performance. \\nAdaline slightly outperformed Perceptron in terms of precision and F1 score. \\nPerceptron had a higher recall but performed poorly in terms of precision, indicating that it made more incorrect predictions. \\nAdaline demonstrated a more balanced performance across the classes, showing that it handles non-linear data slightly better than Perceptron.\\n\\n2.2 Perceptron vs Adaline with 3 Features\\nWith three non-linearly separable features, the pattern from the two-feature test persisted. \\nAdaline consistently outperformed Perceptron in terms of precision and F1 score. \\nPerceptron continued to struggle with precision, while Adaline demonstrated a more balanced handling of the classes. \\nThis suggests that Adaline can better adapt to non-linearly separable data even as the feature set expands.\\n\\n2.3 Perceptron vs Adaline with 4 Features\\nIn the case of four non-linearly separable features, Perceptron's performance deteriorated further, with poor precision and F1 scores. \\nAdaline maintained its advantage with a more balanced classification, showing higher precision and F1 score, but it still struggled with class 1 predictions. \\nOverall, Adaline's better handling of feature complexity and class imbalance in non-linear tasks became more apparent in this case.\\n\\nGeneral Observations and Conclusions\\nLinear Classification:\\n\\nPerceptron showed consistent performance in linearly separable conditions, particularly when using simpler feature combinations (2 and 3 features). However, as the feature complexity increased, its ability to handle class imbalance decreased, leading to poor recall for class 1.\\nAdaline struggled with higher-dimensional linearly separable tasks, particularly when all four features were used. Its complete failure in the 4-feature case suggests that it is less adaptable to high-dimensional linearly separable problems compared to Perceptron.\\nNon-Linear Classification:\\n\\nAdaline consistently outperformed Perceptron in non-linearly separable conditions across all feature combinations. Its superior precision and F1 scores indicate that it handles class imbalance and non-linearity more effectively.\\nPerceptron, on the other hand, struggled significantly with non-linearly separable data. It consistently produced lower precision and F1 scores, suggesting that it is less suited for non-linear classification tasks.\\nImpact of Feature Complexity:\\n\\nIn linearly separable conditions, Perceptron's performance remained relatively stable as the number of features increased, whereas Adaline performed worse with more features.\\nIn non-linearly separable conditions, the increasing number of features did not significantly improve the performance of either model, but Adaline maintained an edge over Perceptron due to its better handling of non-linearity and class imbalance.\\nClass Imbalance Handling:\\n\\nBoth models showed limitations in handling class imbalance, but Adaline demonstrated a better ability to classify imbalanced classes correctly, particularly in non-linear scenarios.\\nFinal Verdict:\\nPerceptron is more stable in linearly separable tasks but struggles significantly with non-linear data and class imbalance, especially as the feature set grows.\\nAdaline performs better in non-linearly separable tasks, showing better adaptability to non-linear conditions and class imbalance. However, it struggles with high-dimensional linearly separable data, especially when using all four features.\\nThe choice between the two models ultimately depends on the task. Perceptron is suitable for simpler, linearly separable tasks, while Adaline is more appropriate for tasks that involve non-linear separability or class imbalance, though it may not scale well with feature complexity in linearly separable cases.\\n\""
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "experiment result and analyze:\n",
    "1. Linear Classification (Linearly Separable Data)\n",
    "1.1 Perceptron vs Adaline with 2 Features\n",
    "In the case of two linearly separable features, both models performed similarly in terms of accuracy. \n",
    "However, Perceptron had a slight edge in precision and F1 score, while Adaline was better at handling class imbalance. \n",
    "Perceptron failed to correctly classify any of the samples from class 1, whereas Adaline managed to classify a portion of class 1 correctly, suggesting that Adaline is slightly more robust in handling imbalanced data.\n",
    "\n",
    "1.2 Perceptron vs Adaline with 3 Features\n",
    "With three features, the performance of both models remained quite similar to the two-feature case. \n",
    "Adaline continued to perform slightly better in handling class imbalance by correctly classifying some class 1 samples. \n",
    "Perceptron maintained its accuracy but still struggled with class 1 classification. \n",
    "This suggests that Adaline shows some degree of flexibility in handling additional features in linearly separable conditions.\n",
    "\n",
    "1.3 Perceptron vs Adaline with 4 Features\n",
    "When testing with all four linearly separable features, Perceptron managed to maintain a relatively stable performance, although its recall for class 1 was quite low, indicating that it struggled to classify class 1 correctly. \n",
    "In contrast, Adaline performed extremely poorly in this scenario, failing to make any correct classifications. This suggests that as the feature space becomes more complex, Perceptron is able to maintain its performance, while Adaline is less capable of handling more dimensions, leading to a complete breakdown in performance.\n",
    "\n",
    "2. Non-Linear Classification (Non-Linearly Separable Data)\n",
    "2.1 Perceptron vs Adaline with 2 Features\n",
    "When dealing with non-linearly separable data with two features, both models saw a decline in performance. \n",
    "Adaline slightly outperformed Perceptron in terms of precision and F1 score. \n",
    "Perceptron had a higher recall but performed poorly in terms of precision, indicating that it made more incorrect predictions. \n",
    "Adaline demonstrated a more balanced performance across the classes, showing that it handles non-linear data slightly better than Perceptron.\n",
    "\n",
    "2.2 Perceptron vs Adaline with 3 Features\n",
    "With three non-linearly separable features, the pattern from the two-feature test persisted. \n",
    "Adaline consistently outperformed Perceptron in terms of precision and F1 score. \n",
    "Perceptron continued to struggle with precision, while Adaline demonstrated a more balanced handling of the classes. \n",
    "This suggests that Adaline can better adapt to non-linearly separable data even as the feature set expands.\n",
    "\n",
    "2.3 Perceptron vs Adaline with 4 Features\n",
    "In the case of four non-linearly separable features, Perceptron's performance deteriorated further, with poor precision and F1 scores. \n",
    "Adaline maintained its advantage with a more balanced classification, showing higher precision and F1 score, but it still struggled with class 1 predictions. \n",
    "Overall, Adaline's better handling of feature complexity and class imbalance in non-linear tasks became more apparent in this case.\n",
    "\n",
    "General Observations and Conclusions\n",
    "Linear Classification:\n",
    "\n",
    "Perceptron showed consistent performance in linearly separable conditions, particularly when using simpler feature combinations (2 and 3 features). However, as the feature complexity increased, its ability to handle class imbalance decreased, leading to poor recall for class 1.\n",
    "Adaline struggled with higher-dimensional linearly separable tasks, particularly when all four features were used. Its complete failure in the 4-feature case suggests that it is less adaptable to high-dimensional linearly separable problems compared to Perceptron.\n",
    "Non-Linear Classification:\n",
    "\n",
    "Adaline consistently outperformed Perceptron in non-linearly separable conditions across all feature combinations. Its superior precision and F1 scores indicate that it handles class imbalance and non-linearity more effectively.\n",
    "Perceptron, on the other hand, struggled significantly with non-linearly separable data. It consistently produced lower precision and F1 scores, suggesting that it is less suited for non-linear classification tasks.\n",
    "Impact of Feature Complexity:\n",
    "\n",
    "In linearly separable conditions, Perceptron's performance remained relatively stable as the number of features increased, whereas Adaline performed worse with more features.\n",
    "In non-linearly separable conditions, the increasing number of features did not significantly improve the performance of either model, but Adaline maintained an edge over Perceptron due to its better handling of non-linearity and class imbalance.\n",
    "Class Imbalance Handling:\n",
    "\n",
    "Both models showed limitations in handling class imbalance, but Adaline demonstrated a better ability to classify imbalanced classes correctly, particularly in non-linear scenarios.\n",
    "Final Verdict:\n",
    "Perceptron is more stable in linearly separable tasks but struggles significantly with non-linear data and class imbalance, especially as the feature set grows.\n",
    "Adaline performs better in non-linearly separable tasks, showing better adaptability to non-linear conditions and class imbalance. However, it struggles with high-dimensional linearly separable data, especially when using all four features.\n",
    "The choice between the two models ultimately depends on the task. Perceptron is suitable for simpler, linearly separable tasks, while Adaline is more appropriate for tasks that involve non-linear separability or class imbalance, though it may not scale well with feature complexity in linearly separable cases.\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc3fa0f-9c91-4dfc-b81e-e09dbcc5c3d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
